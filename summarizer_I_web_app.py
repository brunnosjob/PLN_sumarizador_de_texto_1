#!/usr/bin/env python
# coding: utf-8

# In[217]:


#Importando as bibliotecas de base
import nltk #Tratamento dos textos
import spacy #Tratamento dos textos
import string #Tratamento de strings
import re #Tratamento de caracteres
import numpy #Tratamento num√©rico-matem√°tico-cient√≠fico
import pandas #Cria√ß√£o e manipula√ß√£o de tabelas e series
import heapq #Selecionar melhores senten√ßas
import streamlit as st
from PIL import Image

#Importando m√≥dulos
from nltk import word_tokenize
from IPython.core.display import HTML #Gerar de textos por HTML
from goose3 import Goose #Extrair de textos html
from IPython.core.display import HTML

nltk.download('punkt')

#Aplicando url em vari√°vel
st.markdown('*__Observa√ß√£o: para mais informa√ß√µes acerca do projeto, clique na seta no canto esquerdo superior da tela__*')
st.markdown(' ')

#Informa√ß√µes em sidebar
foto = Image.open('brn.png')
st.sidebar.image(foto, use_column_width=True)
st.sidebar.subheader('Bruno Rodrigues Carloto')
st.sidebar.markdown('Cientista de dados')
st.sidebar.markdown('#### Projeto de portf√≥lio de Processamento de Linguagem Natural')
st.sidebar.markdown('Em breve haver√° artigo descrevendo o passo a passo do projeto. ')
                    
#Cria√ß√£o de p√°ginas
st.sidebar.title('Menu')
pag = st.sidebar.selectbox('Selecione a p√°gina:', ['Experimentar o sumarizador', 'Sobre o sumarizador de texto'])
st.markdown(' ')
                    
st.sidebar.markdown("Redes Sociais :")
st.sidebar.markdown("- [Linkedin](https://www.linkedin.com/in/bruno-rodrigues-carloto)")
st.sidebar.markdown("- [Medium](https://br-cienciadedados.medium.com)")
st.sidebar.markdown("- [Github](https://github.com/brunnosjob)")
                    
#Desenvolvimento das p√°ginas
if pag == 'Experimentar o sumarizador':

    st.subheader('Sumarizador de textos por sublinhamento')
    st.write('#### üë®‚ÄçüöÄ Destaque as partes mais importantes de um texto da internet automaticamente')

    #Criando fun√ß√£o (algoritmo) de gera√ß√£o de resumo
    def sumarizador(url, n_sentencas):
    
        #Criando objeto Goose, extraindo dados HTML e armazenando dados
        g = Goose()
        documento = g.extract(url)

        #Armazenando informa√ß√µes
        titulo = documento.title
        texto_original = documento.cleaned_text
        dominio = documento.domain
    
        #Armazenando stopwords e pontua√ß√µes
        stpwrds = ['de', 'a', 'o', 'que', 'e', '√©', 'do', 'da', 'em', 'um', 'para', 'com', 'n√£o', 'uma', 'os', 'no', 'se', 'na', 'por', 'mais', 'as', 'dos', 'como', 'mas', 'ao', 'ele', 'das', '√†', 'seu', 'sua', 'ou', 'quando', 'muito', 'nos', 'j√°', 'eu', 'tamb√©m', 's√≥', 'pelo', 'pela', 'at√©', 'isso', 'ela', 'entre', 'depois', 'sem', 'mesmo', 'aos', 'seus', 'quem', 'nas', 'me', 'esse', 'eles', 'voc√™', 'essa', 'num', 'nem', 'suas', 'meu', '√†s', 'minha', 'numa', 'pelos', 'elas', 'qual', 'n√≥s', 'lhe', 'deles', 'essas', 'esses', 'pelas', 'este', 'dele', 'tu', 'te', 'voc√™s', 'vos', 'lhes', 'meus', 'minhas', 'teu', 'tua', 'teus', 'tuas', 'nosso', 'nossa', 'nossos', 'nossas', 'dela', 'delas', 'esta', 'estes', 'estas', 'aquele', 'aquela', 'aqueles', 'aquelas', 'isto', 'aquilo', 'estou', 'est√°', 'estamos', 'est√£o', 'estive', 'esteve', 'estivemos', 'estiveram', 'estava', 'est√°vamos', 'estavam', 'estivera', 'estiv√©ramos', 'esteja', 'estejamos', 'estejam', 'estivesse', 'estiv√©ssemos', 'estivessem', 'estiver', 'estivermos', 'estiverem', 'hei', 'h√°', 'havemos', 'h√£o', 'houve', 'houvemos', 'houveram', 'houvera', 'houv√©ramos', 'haja', 'hajamos', 'hajam', 'houvesse', 'houv√©ssemos', 'houvessem', 'houver', 'houvermos', 'houverem', 'houverei', 'houver√°', 'houveremos', 'houver√£o', 'houveria', 'houver√≠amos', 'houveriam', 'sou', 'somos', 's√£o', 'era', '√©ramos', 'eram', 'fui', 'foi', 'fomos', 'foram', 'fora', 'f√¥ramos', 'seja', 'sejamos', 'sejam', 'fosse', 'f√¥ssemos', 'fossem', 'for', 'formos', 'forem', 'serei', 'ser√°', 'seremos', 'ser√£o', 'seria', 'ser√≠amos', 'seriam', 'tenho', 'tem', 'temos', 't√©m', 'tinha', 't√≠nhamos', 'tinham', 'tive', 'teve', 'tivemos', 'tiveram', 'tivera', 'tiv√©ramos', 'tenha', 'tenhamos', 'tenham', 'tivesse', 'tiv√©ssemos', 'tivessem', 'tiver', 'tivermos', 'tiverem', 'terei', 'ter√°', 'teremos', 'ter√£o', 'teria', 'ter√≠amos', 'teriam']
        pontuacoes = ['!"#$%&\'()*+,-/:;<=>?@[\\]^_`{|}~']
    
        tokens = [] #Recebe texto tokenizado
        texto_limpo = [] #Recebe texto pr√©-processado
    
    
        #Remo√ß√£o de espa√ßos, quebras de linha e convertendo mai√∫sculas em min√∫sculas
        texto = re.sub("\n+", "", texto_original)
        texto = re.sub(" +", " ", texto)
        texto = texto.replace("R$", "*RS")
        texto = texto.lower()
    
        #Tokenizando o texto
        for token in texto.split():
            tokens.append(token)
        
        #Eliminando stopwords
        texto_limpo = [elemento for elemento in tokens if elemento not in stpwrds and elemento not in pontuacoes and if elemento not in nltk.isdigit()]
    
        #Unificando texto limpo
        texto_limpo = ' '.join(texto_limpo)
    
        #Gerando frequ√™ncia de palavras
        freq_palavras = nltk.FreqDist([word for word in texto_limpo.split()])
        freq_max = max(freq_palavras.values())
    
        #Gera√ß√£o dos pesos
        for palavra in freq_palavras.keys():
            freq_palavras[palavra] = (freq_palavras[palavra]/freq_max)
        
        #Gerando as notas das senten√ßas
        sentencas_tokenizadas = []
        for sent in texto_original.split('.'):
            sentencas_tokenizadas.append(sent)
            sentencas_tokenizadas = sentencas_tokenizadas + ['.']
        notas_sentencas = {}
        for sentenca in sentencas_tokenizadas:
            for palavra in sentenca.split():
                if palavra in freq_palavras.keys():
                    if sentenca not in notas_sentencas.keys():
                        notas_sentencas[sentenca] = freq_palavras[palavra]
                    else:
                        notas_sentencas[sentenca] += freq_palavras[palavra]
                    
        #Selecionando as melhores senten√ßas
        melhores_sentencas = heapq.nlargest(n_sentencas, notas_sentencas, key=notas_sentencas.get)
    
        #Gerando resumo
        resumo = ' '.join(melhores_sentencas)
    
        #Gerando interface HTML
        texto_html = ''
        st.write(HTML(f"<h1>Resumo sublinhado</h1>"))
        st.write(HTML(f"<footer>*OBS: R$ √© substitu√≠do por rs</footer>"))
        st.write(HTML(f"""<h2>{titulo}<h/>"""))
        for sentenca in sentencas_tokenizadas:
            if sentenca in melhores_sentencas:
                texto_html += str(sentenca).replace(sentenca, f"<mark style='background-color: yellow'>{sentenca}</mark>")
            else:
                texto_html += sentenca
            
        return st.write((HTML(f"""{texto_html}""")))

    #Testando algoritmo
    st.write('#### üåê Cole ou digite o link da p√°gina:')
    url = st.text_input('')
    st.write('#### üìë Digite o total de frases para serem sublinhadas:')
    n_frases = st.number_input(' ', min_value=1, max_value=20)
    if st.button('Gerar sublinhamento'):
        sumarizador(url, n_frases)

elif pag == 'Sobre o sumarizador de texto':
  
  st.subheader('Sobre o sumarizador de texto')
  st.markdown('''
  #### ‚ö†Ô∏è Em desenvolvimento
  
  ''')

